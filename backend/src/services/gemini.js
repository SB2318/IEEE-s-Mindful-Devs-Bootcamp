const {
    GoogleGenerativeAI,
    HarmCategory,
    HarmBlockThreshold,
} = require('@google/generative-ai');

const API_KEY = process.env.GEMINI_API_KEY;

const genAI = new GoogleGenerativeAI(API_KEY);

/**
 * Generates text from a single prompt.
 * @param {string} prompt - The text prompt to send to the model.
 * @returns {Promise<string>} The generated text.
 */
const generateText = async (prompt) => {
    // For text-only input, use the gemini-pro model.
    const model = genAI.getGenerativeModel({ model: 'gemini-2.5-flash-lite' });

    /*
     * == Safety Settings ==
     * Safety settings are crucial for controlling the content generated by the model.
     * They allow you to block content that falls into various categories like
     * harassment, hate speech, sexually explicit content, and dangerous content.
     *
     * Each category can be set to one of several thresholds:
     * - HARM_BLOCK_THRESHOLD_UNSPECIFIED: Default setting.
     * - BLOCK_LOW_AND_ABOVE: Blocks content with a low, medium, or high probability of being harmful.
     * - BLOCK_MEDIUM_AND_ABOVE: Blocks content with a medium or high probability of being harmful.
     * - BLOCK_ONLY_HIGH: Blocks content with a high probability of being harmful.
     * - BLOCK_NONE: Allows all content, regardless of the probability of it being harmful.
    */
    const safetySettings = [
        {
            category: HarmCategory.HARM_CATEGORY_HARASSMENT,
            threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        },
        {
            category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
            threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        },
        {
            category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
            threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        },
        {
            category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
            threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
        },
    ];

    const result = await model.generateContent(prompt, safetySettings);
    const response = await result.response;
    const text = response.text();

    /*
     * == Tokens & Rate Limits ==
     *
     * What is a token?
     * A token is the basic unit of text that a language model processes. It can be a
     * word, a sub-word, or even a single character. For example, the phrase
     * "hello world" might be broken down into two tokens: "hello" and "world".
     * The phrase "eating pizza" might be three tokens: "eat", "ing", and "pizza".
     * The number of tokens in a prompt and its response determines the cost of an
     * API call.
     *
     * What are rate limits?
     * Rate limits are restrictions on how many requests you can make to the API in a
     * given period. For example, the Gemini API might limit you to 60 requests per
     * minute (RPM). If you exceed this limit, your requests will be rejected with
     * an error (e.g., HTTP 429 Too Many Requests).
     *
     * Why are they important?
     * Understanding tokens is key to managing costs. Longer prompts and responses
     * consume more tokens and are therefore more expensive.
     * Respecting rate limits is essential for the stability of your application.
     * If you anticipate high traffic, you may need to implement strategies like
     * exponential backoff for retries or request queuing.
     *
     * In a production environment, you would add robust error handling here to catch
     * rate limit errors and implement a retry strategy.
    */

    return text;
};

/**
 * Engages in a multi-turn chat conversation.
 * @param {Array<object>} history - The conversation history.
 * @param {string} message - The user's latest message.
 * @returns {Promise<string>} The model's response.
 */
const startChat = async (history, message) => {
    const model = genAI.getGenerativeModel({ model: 'gemini-2.5-flash-lite' });

    /*
     * == Multi-turn Chat ==
     * Multi-turn chat allows for a continuous conversation with the model, where it
     * remembers the context of previous messages. This is achieved by sending the
     * entire conversation history with each new message.
     *
     * The 'history' array should contain an alternating sequence of user and model
     * messages, like this:
     * "history": [
     *   { 
     *      "role": "user", parts: [{
     *          "text": "Hello!"
     *      }] 
     *   },
     *   { 
     *      role: "model", 
     *      parts: [{
     *          "text": "Hi there! How can i help you?",
     *      }] 
     *   },
     * ]
    */
    const chat = model.startChat({
        history: history,
        generationConfig: {
            maxOutputTokens: 100,
        },
    });

    const result = await chat.sendMessage(message);
    const response = await result.response;
    const text = response.text();

    return text;
};

module.exports = {
    generateText,
    startChat,
};
